{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gender.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76AbAEyPUDnZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import os\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,BatchNormalization,Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "\n",
        "#inception_weights_path ='https://github.com/fchollet/deep-learning-models/releases/download/v0.2/inception_v3_weights_tf_dim_ordering_tf_kernels.h5'"
      ],
      "metadata": {
        "id": "Yzylc7bhUYlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ChandraShekhar162001/Gender-Classifier-Using-Deep-Learning.git"
      ],
      "metadata": {
        "id": "8QwaK_mfWZul",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca1ba4a5-03a4-49e0-b162-3b21bf3ce536"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Gender-Classifier-Using-Deep-Learning'...\n",
            "remote: Enumerating objects: 58650, done.\u001b[K\n",
            "remote: Total 58650 (delta 0), reused 0 (delta 0), pack-reused 58650\u001b[K\n",
            "Receiving objects: 100% (58650/58650), 261.43 MiB | 34.15 MiB/s, done.\n",
            "Checking out files: 100% (58658/58658), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=50 \n",
        "lr=1e-3\n",
        "batch_size=128\n",
        "data=[]\n",
        "labels=[]"
      ],
      "metadata": {
        "id": "V3jALXBvWc_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size=224 #height/width\n",
        "  "
      ],
      "metadata": {
        "id": "ciLjSbtcWdLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Image Data Generator"
      ],
      "metadata": {
        "id": "O4DBNv1DWz5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_datagen = ImageDataGenerator(horizontal_flip=True,width_shift_range=0.4,height_shift_range=0.4,zoom_range=0.3,rotation_range=20,rescale=1/255)"
      ],
      "metadata": {
        "id": "IZpy2W6OWxq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_gen= ImageDataGenerator(rescale=1/255) "
      ],
      "metadata": {
        "id": "UccASWYgWdVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_size=(size,size)\n",
        "target_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OC0tuhVWdXP",
        "outputId": "4b402d9c-66c1-4973-da61-3b828b6e8eb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator= train_datagen.flow_from_directory(directory='/content/male-female-face-dataset/Training',target_size=target_size,batch_size=batch_size,class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRq-52DkWdaL",
        "outputId": "1b10a152-a04e-4d72-b39b-e045637e5f9b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 47009 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "validation_generator= test_gen.flow_from_directory(directory='/content/male-female-face-dataset/Validation',target_size=target_size,batch_size=batch_size,class_mode='binary')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzblTpODWddC",
        "outputId": "4ecdcef5-2042-407e-a60c-77c938afb026"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 11649 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "231fqaqbYr_1",
        "outputId": "822dea0c-0292-43d6-ff04-0c74207df39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'female': 0, 'male': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_generator.classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUWQTX8XYssp",
        "outputId": "83b23779-6fde-4a3d-e1fb-590d215b399b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47009"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.class_mode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "yt7QP3loYst7",
        "outputId": "5d8eccba-0bc6-4019-f18a-90de63c224d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'binary'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y =  train_generator.next()"
      ],
      "metadata": {
        "id": "bGYY7t-OWdg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek9UIxLpZH4T",
        "outputId": "bb6c2a76-7f93-4138-9a76-f546624f9319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(224, 224, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GG61SpBZMxA",
        "outputId": "611c9efb-d88a-4616-fb51-75237c15f38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.38863635, 0.2756336 , 0.21433403],\n",
              "        [0.36163828, 0.24947925, 0.18986705],\n",
              "        [0.33464026, 0.22332488, 0.16540006],\n",
              "        ...,\n",
              "        [0.3179872 , 0.16368552, 0.10418173],\n",
              "        [0.31764707, 0.16470589, 0.10588236],\n",
              "        [0.31764707, 0.16470589, 0.10588236]],\n",
              "\n",
              "       [[0.41176474, 0.29803923, 0.23529413],\n",
              "        [0.41176474, 0.29803923, 0.23529413],\n",
              "        [0.41176474, 0.29803923, 0.23529413],\n",
              "        ...,\n",
              "        [0.31764707, 0.16470589, 0.10588236],\n",
              "        [0.31764707, 0.16470589, 0.10588236],\n",
              "        [0.31764707, 0.16470589, 0.10588236]],\n",
              "\n",
              "       [[0.5882353 , 0.4666667 , 0.39607847],\n",
              "        [0.56629866, 0.44570497, 0.3760917 ],\n",
              "        [0.5283326 , 0.40942633, 0.34150043],\n",
              "        ...,\n",
              "        [0.31764707, 0.16470589, 0.10588236],\n",
              "        [0.31764707, 0.16470589, 0.10588236],\n",
              "        [0.31283605, 0.16133817, 0.10299574]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0.9843138 , 0.91372555, 0.86666673],\n",
              "        [0.9843138 , 0.91372555, 0.86666673],\n",
              "        [0.9843138 , 0.91372555, 0.86666673],\n",
              "        ...,\n",
              "        [0.48261407, 0.2630062 , 0.20174235],\n",
              "        [0.48627454, 0.26666668, 0.20784315],\n",
              "        [0.48627454, 0.26666668, 0.20784315]],\n",
              "\n",
              "       [[0.9843138 , 0.91372555, 0.86666673],\n",
              "        [0.9843138 , 0.91372555, 0.86666673],\n",
              "        [0.9843138 , 0.91372555, 0.86666673],\n",
              "        ...,\n",
              "        [0.48622203, 0.26661417, 0.20775564],\n",
              "        [0.48627454, 0.26666668, 0.20784315],\n",
              "        [0.48627454, 0.26666668, 0.20784315]],\n",
              "\n",
              "       [[0.9843138 , 0.91372555, 0.86666673],\n",
              "        [0.9843138 , 0.91372555, 0.86666673],\n",
              "        [0.9843138 , 0.91372555, 0.86666673],\n",
              "        ...,\n",
              "        [0.48627454, 0.26666668, 0.20784315],\n",
              "        [0.48627454, 0.26666668, 0.20784315],\n",
              "        [0.48627454, 0.26666668, 0.20784315]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build ML Model\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7j2ZDRDUZZA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model= Sequential()\n",
        "model.add(InceptionV3(include_top=False,pooling='avg',weights='imagenet'))\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(2048,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.layers[0].trainable=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v14myN-jZQGW",
        "outputId": "ce96709d-5733-437d-bd4e-46250e9eaa76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 0s 0us/step\n",
            "87924736/87910968 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E5TJSh2ZQCr",
        "outputId": "ad29aaf4-79a4-47ef-a5c2-337cddb45110"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inception_v3 (Functional)   (None, 2048)              21802784  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " batch_normalization_94 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2048)              4196352   \n",
            "                                                                 \n",
            " batch_normalization_95 (Bat  (None, 2048)             8192      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              2098176   \n",
            "                                                                 \n",
            " batch_normalization_96 (Bat  (None, 1024)             4096      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 1025      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 28,118,817\n",
            "Trainable params: 6,305,793\n",
            "Non-trainable params: 21,813,024\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BnDHMs7wawq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len((train_generator.filenames)),batch_size,len((train_generator.filenames))//batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCgaZ-hjbU9N",
        "outputId": "8d569335-121f-42ab-9775-63b17bece1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(47009, 128, 367)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator,steps_per_epoch=len(train_generator.filenames)//batch_size,\n",
        "          epochs=2,validation_data=validation_generator,validation_steps=len(validation_generator.filenames)//batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pgiBX4hZQBR",
        "outputId": "331d5dd5-d180-4d4a-b7cd-6b4a40049430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "367/367 [==============================] - 546s 1s/step - loss: 0.3791 - accuracy: 0.8463 - val_loss: 0.2129 - val_accuracy: 0.9184\n",
            "Epoch 2/2\n",
            "212/367 [================>.............] - ETA: 3:34 - loss: 0.3151 - accuracy: 0.8650"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Model"
      ],
      "metadata": {
        "id": "4uaE2rA1cQUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path='/content/male-female-face-dataset/Validation/female/112944.jpg.jpg'"
      ],
      "metadata": {
        "id": "TBufq6ZkZP8O"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from tensorflow.keras.preprocessing.image import load_img,img_to_array"
      ],
      "metadata": {
        "id": "jhKJdU4lZPwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = load_img(img_path,target_size=(size,size,3))\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "bIJMtHwLcz5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=img_to_array(img)"
      ],
      "metadata": {
        "id": "4v_XKXzQdA4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=img/255.0"
      ],
      "metadata": {
        "id": "h2MtkBSUdIZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img=img.reshape(1,size,size,3)"
      ],
      "metadata": {
        "id": "w0YPKhAJdOyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "id": "ta21TFOjdV_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict(img)"
      ],
      "metadata": {
        "id": "mRM2HCNBdZ4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.class_indices"
      ],
      "metadata": {
        "id": "uUWzJTGHdcsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_classes(data):\n",
        "  prob=model.predict(data)[0][0]\n",
        "\n",
        "\n",
        "  if prob<0.5:\n",
        "    return 'female',(1- prob)\n",
        "  else:\n",
        "    return 'male',prob"
      ],
      "metadata": {
        "id": "NGvEHBSkdcqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_classes(img)"
      ],
      "metadata": {
        "id": "59qXmB6udcnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Real-Time Prediction using Web-Cam\n"
      ],
      "metadata": {
        "id": "sGkiHXK1fdjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode"
      ],
      "metadata": {
        "id": "rY43vikYdclN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "T-VpZ0hOebMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename=take_photo()\n",
        "  print('Saved to {}'.format(filename))\n",
        "\n",
        "  #show the image which was just taken.\n",
        "  display(Image(filename))\n",
        "except Exception as err:\n",
        "\n",
        "  print(str(err))"
      ],
      "metadata": {
        "id": "FnLOAWg3ebJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(img_path):\n",
        "  img=load_img(img_path,target_size=(size,size,3))\n",
        "  plt.imshow(img)\n",
        "  img=img_to_array(img)\n",
        "  img=img/255.0\n",
        "  img=img.reshape(1,size,size,3)\n",
        "\n",
        "  pred,prob = get_classes(img)\n",
        "  return pred,prob"
      ],
      "metadata": {
        "id": "bam1PyzKeaym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_prediction('/content/male-female-face-dataset/Validation/male/063438.jpg.jpg')"
      ],
      "metadata": {
        "id": "dwRSsp0adcgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SZj6tVjddcea"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}